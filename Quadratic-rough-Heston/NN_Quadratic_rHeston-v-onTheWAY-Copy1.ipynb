{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "# import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('Data')\n",
    "# os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xef in position 129: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1371320863af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"impliedVols_total.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0myy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\jia21\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst_vals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m                 \u001b[0mfirst_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m                 \u001b[0mfirst_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xef in position 129: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "df= np.loadtxt(\"impliedVols_total.txt\")\n",
    "\n",
    "xx = df[:, :6]\n",
    "yy = df[:, 6:]\n",
    "\n",
    "# # total = np.zeros((len(yy), 6+strikes_dim*maturities_dim))\n",
    "# # total[:,:6] = xx\n",
    "# # for i in range(maturities_dim):\n",
    "# #     print(total[:, 6+i*strikes_dim:((i+1)*strikes_dim - 1)].shape,yy[:, i*(strikes_dim-1):((i+1)*(strikes_dim - 1))].shape)\n",
    "# #     total[:, 6+i*strikes_dim:(6+(i+1)*strikes_dim - 1)] = yy[:, i*(strikes_dim-1):((i+1)*(strikes_dim-1))]\n",
    "# #     total[:, 6+(i+1)*strikes_dim-1] = df_[:, i]\n",
    "# print(len(df123), len(df123[0]))\n",
    "# np.savetxt(\"parameters_impliedVols_T7K15.txt\", df123, fmt='%.4f')       \n",
    "len(df), len(df[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maturities = np.array([0.05, 0.1, 0.2, 0.3, 0.5, 0.75, 1.0])# Time to expiry in years\n",
    "# strikes = np.array([80, 85, 90, 95, 97, 99, 100, 101, 103, 105, 110, 115, 120, 122])\n",
    "strikes = np.array([82., 85., 90., 95., 98., 100., 102., 105., 110., 115., 122.])\n",
    "# strikes = np.array([5.25, 5.50, 5.75, 6.00, 6.25])\n",
    "logmoneyness = np.log(strikes/100.)\n",
    "strikes_dim = len(strikes)\n",
    "maturities_dim = len(maturities)\n",
    "strikes_dim, maturities_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(np.min(df[:, i]),np.max(df[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(yy == yy.min())\n",
    "# yy[0][12]\n",
    "yy.min(axis=0)\n",
    "# np.where(yy == yy.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove invalid paramters\n",
    "invalid = np.where(yy == yy.min())[0]\n",
    "xx = np.delete(xx, (invalid), axis=0)\n",
    "yy = np.delete(yy, (invalid), axis=0)\n",
    "xx.shape, yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(np.min(xx[:, i]),np.max(xx[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Generation\n",
    "\n",
    "\n",
    "### **Inputs: parameters in the rough quadratic Heston model** \n",
    "####   1.  $\\alpha\\in(0.5, 0.7);$\n",
    "####   2. $\\lambda\\in(0.5, 1.5);$\n",
    "####   3. $a\\in(0.1, 0.75);$\n",
    "####   4. $b\\in(0.05,0.5);$\n",
    "####   5. $c\\in(0.0001, 0.01);$\n",
    "####   6. $Z_0\\in(0.05, 0.3).$  \n",
    "\n",
    "    \n",
    "### **Outputs:**\n",
    "#### Implied volatility surface (14x9=126 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling:\n",
    "\n",
    "### Model Parameters $\\theta\\to scale(\\theta)\\in[0,1]$\n",
    "$$scale(\\theta^i)=\\frac{(\\theta^i-\\theta^i_{min})}{\\theta^i_{max}-\\theta^i_{min}},\\quad i \\in |\\Theta|$$\n",
    "\n",
    "### Implied volatilities\n",
    "$$scale(\\sigma_{BS}^{i,j})=\\frac{\\sigma_{BS}^{i,j}-\\mathbb{E}[\\sigma_{BS}^{i,j}]}{std(\\sigma_{BS}^{i,j})},\\quad i\\in\\text{Maturities},\\; j\\in \\text{Strikes}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    xx, yy, test_size=0.15, random_state=42)\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# scale = StandardScaler()\n",
    "# y_train_transform = scale.fit_transform(y_train)\n",
    "# y_test_transform = scale.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha\\in[0.505, 0.7],\\; \\lambda\\in[0.5, 1.5],\\; a\\in[0.1, 0.75],\\; b\\in[0.05, 0.5],\\; c\\in[0.0001, 0.01],\\; Z_0\\in[0.05, 0.3]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub=np.max(xx, axis=0)\n",
    "lb=np.min(xx, axis=0)\n",
    "print(ub, lb)\n",
    "def myscale(x):\n",
    "    res=np.zeros(6)\n",
    "    for i in range(6):\n",
    "        res[i]=(x[i] - (ub[i] + lb[i])*0.5) * 2 / (ub[i] - lb[i])\n",
    "        \n",
    "    return res\n",
    "def myinverse(x):\n",
    "    res=np.zeros(6)\n",
    "    for i in range(6):\n",
    "        res[i]=x[i]*(ub[i] - lb[i]) *0.5 + (ub[i] + lb[i])*0.5\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transform = np.array([myscale(x) for x in X_train])\n",
    "x_test_transform = np.array([myscale(x) for x in X_test])\n",
    "x_test_transform.shape\n",
    "# np.max(x_train_transform, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Create the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "input1 = keras.layers.Input(shape=(6,))\n",
    "x1 = keras.layers.Dense(30,activation = 'elu')(input1)\n",
    "x2=keras.layers.Dense(30,activation = 'elu')(x1) \n",
    "x3=keras.layers.Dense(30,activation = 'elu')(x2) \n",
    "\n",
    "\n",
    "x4=keras.layers.Dense(77,activation = 'linear')(x3)\n",
    "\n",
    "\n",
    "modelGEN = keras.models.Model(inputs=input1, outputs=x4)\n",
    "modelGEN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Fit the Neural Network (No need to run the code, parameters are stored on a .h5 file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "modelGEN.compile(loss = root_mean_squared_error, optimizer = \"adam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGEN.fit(x_train_transform, y_train, \n",
    "             batch_size=32,\n",
    "#              validation_split = 0.2,\n",
    "             validation_data = (x_test_transform, y_test),\n",
    "             epochs = 200, verbose = True,shuffle=1)#,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store/Load optimal NN parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGEN.save_weights('qrHestonNNWeights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGEN.load_weights('qrHestonNNWeights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNParameters=[]\n",
    "for i in range(1,len(modelGEN.layers)):\n",
    "    NNParameters.append(modelGEN.layers[i].get_weights())\n",
    "# np.savetxt(\"nnParameters.txt\", np.asarray(NNParameters))\n",
    "# NNParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Optimisation\n",
    "### (Numpy Implementation of the Neural Network tends to be faster than Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the *elu* function writes:\n",
    "$$elu(x) = \\begin{cases}e^{x}-1,\\; x<0\\\\\n",
    "x,\\; x\\geq 0.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumLayers=3\n",
    "def elu(x):\n",
    "    #Careful function ovewrites x\n",
    "    ind=(x<0)\n",
    "    x[ind]=np.exp(x[ind])-1\n",
    "    return x\n",
    "def eluPrime(y):\n",
    "    # we make a deep copy of input x\n",
    "    x=np.copy(y)\n",
    "    ind=(x<0)\n",
    "    x[ind]=np.exp(x[ind])\n",
    "    x[~ind]=1\n",
    "    return x\n",
    "def NeuralNetwork(x):\n",
    "    input1=x\n",
    "    for i in range(NumLayers):\n",
    "        input1=np.dot(input1,NNParameters[i][0])+NNParameters[i][1]\n",
    "        #Elu activation\n",
    "        input1=elu(input1)\n",
    "    #The output layer is linnear\n",
    "    i+=1\n",
    "    return np.dot(input1,NNParameters[i][0])+NNParameters[i][1]\n",
    "def NeuralNetworkGradient(x):\n",
    "    input1=x\n",
    "    #Identity Matrix represents Jacobian with respect to initial parameters\n",
    "    grad=np.eye(6)\n",
    "    #Propagate the gradient via chain rule\n",
    "    for i in range(NumLayers):\n",
    "        input1=(np.dot(input1,NNParameters[i][0])+NNParameters[i][1])\n",
    "        grad=(np.einsum('ij,jk->ik', grad, NNParameters[i][0]))\n",
    "        #Elu activation\n",
    "        grad*=eluPrime(input1)\n",
    "        input1=elu(input1)\n",
    "    #input1.append(np.dot(input1[i],NNParameters[i+1][0])+NNParameters[i+1][1])\n",
    "#     grad=np.einsum('ij,jk->ik',grad,NNParameters[i+1][0])\n",
    "    grad=np.dot(grad,NNParameters[i+1][0])\n",
    "    #grad stores all intermediate Jacobians, however only the last one is used here as output\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Implied vol relative errors on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork(X_sample[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AVERAGE VALUES #######\n",
    "X_sample = x_test_transform\n",
    "y_sample = y_test\n",
    "\n",
    "# prediction=[scale.inverse_transform(modelGEN.predict(X_sample[i].reshape(1,6))[0]) for i in range(len(X_sample))]\n",
    "# prediction=[modelGEN.predict(X_sample[i].reshape(1,6))[0] for i in range(len(X_sample))]\n",
    "prediction=[NeuralNetwork(X_sample[i]) for i in range(len(X_sample))]\n",
    "\n",
    "\n",
    "plt.figure(1,figsize=(14,4))\n",
    "ax=plt.subplot(1,3,1)\n",
    "err = np.mean(100*np.abs((y_sample-prediction)/y_sample),axis = 0)\n",
    "RMSE = np.sqrt(np.mean(np.power(y_sample-prediction, 2), axis=1)) ### compute the RMSE of the calibrated smile surface\n",
    "\n",
    "plt.title(\"Average relative error\",fontsize=15,y=1.04)\n",
    "plt.imshow(err.reshape(maturities_dim,strikes_dim))\n",
    "plt.colorbar(format=mtick.PercentFormatter())\n",
    "\n",
    "ax.set_xticks(np.linspace(0,len(strikes)-1,len(strikes)))\n",
    "ax.set_xticklabels(strikes)\n",
    "ax.set_yticks(np.linspace(0,len(maturities)-1,len(maturities)))\n",
    "ax.set_yticklabels(maturities)\n",
    "plt.xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "plt.ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "\n",
    "ax=plt.subplot(1,3,2)\n",
    "err = 100*np.std(np.abs((y_sample-prediction)/y_sample),axis = 0)\n",
    "plt.title(\"Std relative error\",fontsize=15,y=1.04)\n",
    "plt.imshow(err.reshape(maturities_dim,strikes_dim))\n",
    "plt.colorbar(format=mtick.PercentFormatter())\n",
    "ax.set_xticks(np.linspace(0,len(strikes)-1,len(strikes)))\n",
    "ax.set_xticklabels(strikes)\n",
    "ax.set_yticks(np.linspace(0,len(maturities)-1,len(maturities)))\n",
    "ax.set_yticklabels(maturities)\n",
    "plt.xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "plt.ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "\n",
    "ax=plt.subplot(1,3,3)\n",
    "err = 100*np.max(np.abs((y_sample-prediction)/y_sample),axis = 0)\n",
    "plt.title(\"Maximum relative error\",fontsize=15,y=1.04)\n",
    "plt.imshow(err.reshape(maturities_dim,strikes_dim))\n",
    "plt.colorbar(format=mtick.PercentFormatter())\n",
    "ax.set_xticks(np.linspace(0,len(strikes)-1,len(strikes)))\n",
    "ax.set_xticklabels(strikes)\n",
    "ax.set_yticks(np.linspace(0,len(maturities)-1,len(maturities)))\n",
    "ax.set_yticklabels(maturities)\n",
    "plt.xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "plt.ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('qrHestonNNErrors.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot sample Smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    sample_ind = i+250\n",
    "    X_sample = x_test_transform[sample_ind]\n",
    "    y_sample = y_test[sample_ind]\n",
    "    print(X_test[sample_ind])\n",
    "    prediction=modelGEN.predict(X_sample.reshape(1,6))[0]\n",
    "#     prediction=scale.inverse_transform(modelGEN.predict(X_sample.reshape(1,6))[0])\n",
    "    plt.figure(1,figsize=(14,6))\n",
    "    for i in range(maturities_dim):\n",
    "        plt.subplot(2,4,i+1)\n",
    "\n",
    "        plt.plot(logmoneyness, y_sample[i*strikes_dim:(i+1)*strikes_dim],'b',label=\"Input data\")\n",
    "        plt.plot(logmoneyness, prediction[i*strikes_dim:(i+1)*strikes_dim],'--r',label=\" NN Approx\")\n",
    "\n",
    "\n",
    "        plt.title(\"Maturity=%1.2f \"%maturities[i])\n",
    "        plt.xlabel(\"log-moneyness\")\n",
    "        plt.ylabel(\"Implied vol\")\n",
    "\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "#     time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha\\in[0.505, 0.7],\\; \\lambda\\in[0.5, 1.5],\\; a\\in[0.1, 0.75],\\; b\\in[0.05, 0.5],\\; c\\in[0.0001, 0.01],\\; Z_0\\in[0.05, 0.3]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use gradient methods for optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunc(x,sample_ind):\n",
    "    return np.sum(np.power((NeuralNetwork(x)-y_test[sample_ind]),2))\n",
    "def Jacobian(x,sample_ind):\n",
    "    return 2*np.sum((NeuralNetwork(x)-y_test[sample_ind])*NeuralNetworkGradient(x),axis=1)\n",
    "#Cost Function for Levenberg Marquardt\n",
    "def CostFuncLS(x,sample_ind):\n",
    "    return (NeuralNetwork(x)-y_test[sample_ind])\n",
    "\n",
    "def JacobianLS(x,sample_ind):\n",
    "    return NeuralNetworkGradient(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Approx=[]\n",
    "Timing = []\n",
    "\n",
    "solutions=np.zeros(6)\n",
    "init=lb\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    disp=str(i+1)+\"/\" + str(len(X_test))\n",
    "    print (disp, end=\"\\r\")\n",
    "    #Levenberg-Marquardt\n",
    "    start= time.time()\n",
    "    I=scipy.optimize.least_squares(CostFuncLS, init, JacobianLS,method='lm',args=(i,),gtol=1E-10)\n",
    "    end= time.time()\n",
    "    solutions=myinverse(I.x)\n",
    "    times=end-start\n",
    "    \n",
    "    Approx.append(np.copy(solutions))\n",
    "    Timing.append(np.copy(times))\n",
    "    \n",
    "print(\"time spent: %ss\"%np.round(np.sum(Timing), 6))\n",
    "# print(\"solution: \", np.round(np.mean(Approx, axis=0), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Lavenberg-Marquardt Optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMParameters=np.array([Approx[i] for i in range(len(Approx))])\n",
    "np.savetxt(\"NNParametersQuadraticRoughHeston.txt\",LMParameters)\n",
    "LMParameters = np.loadtxt(\"NNParametersQuadraticRoughHeston.txt\")\n",
    "LMParameters.shape\n",
    "np.mean(LMParameters, axis=0)\n",
    "# Approx[0][0], Approx[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Errors with Levenberg-Marquardt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[\"$\\\\alpha$\",\"$\\\\lambda$\",\"$a$\",\"$b$\", \"$c$\",\"$Z_0$\"]\n",
    "average=np.zeros([6,len(X_test)])\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "for u in range(6):\n",
    "    ax=plt.subplot(2,3,u+1)\n",
    "    x=X_test[:len(X_test),u]\n",
    "#     print(abs(LMParameters[:1800,u]))\n",
    "#     print(100*np.abs(LMParameters[:1800,u]))\n",
    "    plt.plot(x,100*np.abs(LMParameters[:len(X_test),u]-x)/np.abs(x),'b*')\n",
    "    average[u,:]=np.abs(LMParameters[:len(X_test),u]-x)/np.abs(x)\n",
    "    \n",
    "    plt.title(titles[u],fontsize=20)\n",
    "    plt.ylabel('relative Error',fontsize=15)\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    plt.text(0.5, 0.8, 'Average: %1.2f%%\\n Median:   %1.2f%% '%(np.mean(100*average[u,:]),np.quantile(100*average[u,:],0.5)), horizontalalignment='center',verticalalignment='center', transform=ax.transAxes,fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('qrHestonParameterRelativeErrors.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[\"$\\\\alpha$\",\"$\\\\lambda$\",\"$a$\",\"$b$\", \"$c$\",\"$Z_0$\"]\n",
    "average=np.zeros([6,len(X_test)])\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "for u in range(6):\n",
    "    ax=plt.subplot(2,3,u+1)\n",
    "    y=X_test[:len(X_test),u]\n",
    "    plt.plot(y,np.abs(LMParameters[:len(X_test),u]-y),'b*')\n",
    "    average[u,:]=np.abs(LMParameters[:len(X_test),u]-y)\n",
    "    \n",
    "    plt.title(titles[u],fontsize=20)\n",
    "    plt.ylabel('Absolute Error',fontsize=15)\n",
    "    #plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    plt.text(0.5, 0.8, 'Average: %1.2f%%\\n Median:   %1.2f%% '%(np.mean(average[u,:]),np.quantile(average[u,:],0.5)), horizontalalignment='center',verticalalignment='center', transform=ax.transAxes,fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('qrHestonParameterAbsoluteErrors.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.clf()\n",
    "plt.subplot(121)\n",
    "ax = plt.gca()\n",
    "q=np.linspace(0,0.99,200)\n",
    "for u in range(6):\n",
    "    p=plt.plot(100*q,np.quantile(100*average[u,:],q),label=titles[u])\n",
    "    \n",
    "    c=p[0].get_color()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.set_xlim(0,100)\n",
    "plt.plot(100*np.ones(2)*0.95,np.array([0,ymax]),'--k',label=\"95% quantile\")\n",
    "plt.title(\"Empirical CDF of parameter relative error\",fontsize=20)\n",
    "plt.legend(fontsize=17)\n",
    "plt.xlabel(\"quantiles\",fontsize=17)\n",
    "plt.ylabel(\"relative error\",fontsize=17)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "plt.tick_params(axis='both', which='major', labelsize=17)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=17)\n",
    "plt.xticks(np.arange(0, 101, step=10))\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "ax = plt.gca()\n",
    "q=np.linspace(0,1,200)\n",
    "p=plt.plot(100*q,np.quantile(100*RMSE,q),linewidth=3,label=\"RMSE\")\n",
    "ymin, ymax = ax.get_ylim()\n",
    "plt.plot(100*np.ones(2)*0.99,np.array([0,ymax]),'--k',label=\"99% quantile\")\n",
    "plt.title(\"Empirical CDF of implied vol surface RMSE\",fontsize=20)\n",
    "plt.legend(fontsize=17)\n",
    "plt.xlabel(\"quantiles\",fontsize=17)\n",
    "plt.ylabel(\"RMSE\",fontsize=17)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "plt.tick_params(axis='both', which='major', labelsize=17)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=17)\n",
    "plt.xticks(np.arange(0, 101, step=10))\n",
    "plt.grid()\n",
    "plt.savefig('qrHestonErrorCDF.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration on market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market logmoneyness:  [-0.118, -0.097, -0.077, -0.056, -0.008, 0.039, 0.083, 0.126, 0.167]\n"
     ]
    }
   ],
   "source": [
    "from scipy import  interpolate\n",
    "from scipy.interpolate import griddata\n",
    "import json\n",
    "\n",
    "df_logmoneyness_market = json.load(open('logmoneyness_market.json'))\n",
    "df_market = json.load(open('impliedVols_market.json'))\n",
    "### maturities and strikes on the market\n",
    "maturities_market = [0.06, 0.15, 0.31, 0.56]\n",
    "print(\"Market logmoneyness: \",df_logmoneyness_market[10])\n",
    "# print(\"Model logmoneyness: \",np.round(logmoneyness,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_NN(x):\n",
    "    \"\"\"\n",
    "    a function for the interpolation of the market data to the NeuralNetwork\n",
    "    Input: \n",
    "        x: the input sample for the Neural Network function\n",
    "        logmonyness_market(dim=5), maturities_market(dim=4)\n",
    "    Output: \n",
    "        f: the interpolation results of the NeuralNetwork, \n",
    "        df: the interpolation results of the NeuralNetworkGradient\n",
    "    \"\"\"\n",
    "    logKs, Ts   = np.meshgrid(logmoneyness, maturities)\n",
    "    logKs_, Ts_ = np.meshgrid(logmoneyness_market, maturities_market)\n",
    "#     points      = np.vstack((logKs.ravel(), Ts.ravel())).T\n",
    "#     f           = griddata(points, NeuralNetwork(x),(logKs_.ravel(), Ts_.ravel()), method='cubic')\n",
    "#     sbs = interpolate.SmoothBivariateSpline(logKs.ravel(), Ts.ravel()NNParameters NeuralNetwork(x))\n",
    "    print(NeuralNetwork(x).shape)\n",
    "\n",
    "    sbs = interpolate.RectBivariateSpline(maturities, logmoneyness, NeuralNetwork(x).reshape(maturities_dim,-1))\n",
    "\n",
    "    fnew = sbs.ev(maturities_market, logmoneyness_market)                         \n",
    "    return fnew\n",
    "\n",
    "def interpolation_NN_Gradient(x):\n",
    "    \"\"\"\n",
    "    a function for the interpolation of the market data to the NeuralNetworkGradient\n",
    "    Input: \n",
    "        x: the input sample for the Neural Network function\n",
    "        logmonyness_market(dim=5), maturities_market(dim=4)\n",
    "    Output: \n",
    "        gradient: the interpolation results of the NeuralNetworkGradient\n",
    "    \"\"\"\n",
    "    logKs, Ts   = np.meshgrid(logmoneyness, maturities)\n",
    "    logKs_, Ts_ = np.meshgrid(logmoneyness_market, maturities_market)\n",
    "    \n",
    "    gradient    = np.zeros((6, len(logmoneyness_market)*len(maturities_market)))\n",
    "    nnGradient  = NeuralNetworkGradient(x)\n",
    "    \n",
    "    for i in range(6):\n",
    "        nnGradient[i].reshape(maturities_dim,-1).shape\n",
    "        sbs = interpolate.RectBivariateSpline(maturities, logmoneyness, nnGradient[i].reshape(maturities_dim,-1))\n",
    "        gradient[i] = sbs.ev(maturities_market, logmoneyness_market)  \n",
    "    \n",
    "#     for i in range(6):\n",
    "#         df = interp2d(logKs, Ts, nnGradient[i])\n",
    "#         gradient[i] = df(logmonyness_market, maturities_market).ravel()\n",
    "    return gradient\n",
    "    \n",
    "\n",
    "# def interpolation_NN_Gradient(x):\n",
    "#     \"\"\"\n",
    "#     a function for the interpolation of the market data to the NeuralNetworkGradient\n",
    "#     Input: \n",
    "#         x: the input sample for the Neural Network function\n",
    "#         logmonyness_market(dim=5), maturities_market(dim=4)\n",
    "#     Output: \n",
    "#         gradient: the interpolation results of the NeuralNetworkGradient\n",
    "#     \"\"\"\n",
    "#     logKs, Ts   = np.meshgrid(logmoneyness, maturities)\n",
    "#     logKs_, Ts_ = np.meshgrid(logmoneyness_market, maturities_market)\n",
    "#     points      = np.vstack((logKs.ravel(), Ts.ravel())).T\n",
    "#     fnew = interpolate.bisplev(logmoneyness_market, maturities_market)\n",
    "#     gradient    = np.zeros((6, len(logmoneyness_market)*len(maturities_market)))\n",
    "#     nnGradient  = NeuralNetworkGradient(x)\n",
    "    \n",
    "#     for i in range(6):\n",
    "#         gradient[i] = griddata(points, nnGradient[i],(logKs_.ravel(), Ts_.ravel()), method='cubic')\n",
    "    \n",
    "# #     for i in range(6):\n",
    "# #         df = interp2d(logKs, Ts, nnGradient[i])\n",
    "# #         gradient[i] = df(logmonyness_market, maturities_market).ravel()\n",
    "#     return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_NN(X_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunc(x,sample_ind):\n",
    "    prediction = interpolation_NN(x)\n",
    "    return np.sum(np.power((prediction - df_market[sample_ind]),2))\n",
    "\n",
    "def Jacobian(x,sample_ind):\n",
    "    prediction = interpolation_NN(x)\n",
    "    gradient = interpolation_NN_Gradient(x)\n",
    "    \n",
    "    return 2*np.sum((prediction - df_market[sample_ind])*gradient,axis=1)\n",
    "\n",
    "# #Cost Function for Levenberg Marquardt\n",
    "\n",
    "def CostFuncLS(x,sample_ind):\n",
    "    prediction = interpolation_NN(x)\n",
    "    return (prediction - df_market[sample_ind])\n",
    "\n",
    "def JacobianLS(x,sample_ind):\n",
    "    return interpolation_NN_Gradient(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnds = ((-1,1), (-1,1),(-1,1), (-1,1),(-1,1), (-1,1))\n",
    "type(bnds)\n",
    "np.max([i[-1] for i in df_logmoneyness_market]), np.log(122/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approx=[]\n",
    "Timing = []\n",
    "\n",
    "solutions=np.zeros([3,6])\n",
    "init=[(ub[i]+lb[i])*0.5 for i in range(6)]\n",
    "# init = lb\n",
    "# init = np.array([0.51, 1.2, 0.384, 0.095, 0.0025, 0.1])\n",
    "times = np.zeros(3)\n",
    "for i in range(len(df_market)):\n",
    "    disp=str(i+1)+\"/\" + str(len(df_market))\n",
    "    print (disp, end=\"\\r\")\n",
    "    logmoneyness_market = df_logmoneyness_market[i]\n",
    "    \n",
    "    start= time.time()\n",
    "#     I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='SLSQP', jac=Jacobian,bounds=bnds, tol=1E-10,options={\"maxiter\":5000})\n",
    "    I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='SLSQP', jac=Jacobian, tol=1E-10,options={\"maxiter\":5000})\n",
    "    end= time.time()\n",
    "    solutions[0,:]=myinverse(I.x)\n",
    "    times[0]=end-start\n",
    "    \n",
    "    start= time.time()\n",
    "    I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='L-BFGS-B', jac=Jacobian, tol=1E-10,options={\"maxiter\":5000})\n",
    "#     I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='L-BFGS-B', jac=Jacobian,bounds=bnds, tol=1E-10,options={\"maxiter\":5000})\n",
    "    end= time.time()\n",
    "    solutions[1,:]=myinverse(I.x)\n",
    "# #     print(solutions[1,:]) \n",
    "    times[1]=end-start\n",
    "    \n",
    "   # Levenberg-Marquardt\n",
    "    start= time.time()\n",
    "    I=scipy.optimize.least_squares(CostFuncLS, x0=init,jac=JacobianLS,method='lm', args=(i,),gtol=1E-10)\n",
    "#     I=scipy.optimize.least_squares(CostFuncLS, x0=init,jac=JacobianLS,bounds=(-np.ones(6), np.ones(6)), args=(i,),gtol=1E-10)\n",
    "    end= time.time()\n",
    "    solutions[2, :]=myinverse(I.x)\n",
    "    times[2] = end-start\n",
    "    \n",
    "    Approx.append(np.copy(solutions))\n",
    "    Timing.append(np.copy(times))\n",
    "    \n",
    "print(\"time spent: %ss\"%np.round(np.sum(Timing, axis=0), 6))\n",
    "# print(\"solution: \", np.round(np.mean(Approx, axis=0), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(40):\n",
    "#     print(np.mean(Approx[i][1]-Approx[i][2]))\n",
    "np.mean([np.power(Approx[i][0]-Approx[i][2], 2) for i in range(len(Approx))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with bounds\n",
    "for j in range(3):\n",
    "    tmp = [Approx[i][j] for i in range(len(Approx))]\n",
    "    print(\"Mean:\",np.mean(tmp, axis=0))\n",
    "    print(\"Std: \",np.round(np.std(tmp, axis=0), 10))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "jia21",
   "language": "python",
   "name": "jia21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {
    "1b7631c6baf748d1986bebf20535c17f": {
     "views": [
      {
       "cell_index": 30
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
